\documentclass[10pt]{article}
\usepackage[T1]{fontenc}

% Document Details
\newcommand{\CLASS}{CSE 521}
\newcommand{\assigmentnum}{Problem Set 1}

\usepackage[margin = 1.5in]{geometry}
\input{../../TeX_headers/title.tex} % Title Styling
\input{../../TeX_headers/styling.tex} % General Styling
\input{../../TeX_headers/code.tex} % Code Display Setup
\input{../../TeX_headers/math.tex} % Math shortcuts
\input{../../TeX_headers/proof.tex} % Proof shortcuts
\input{../../TeX_headers/problem.tex} % Problem Environment

\rhead{\sffamily Tyler Chen \textbf{\thepage}}


\begin{document}
\maketitle

\begin{problem}[Problem 1]
In this problem you are supposed to implement Karger’s min-cut algorithm. I have uploaded three input
files to the course website. Each file contains the list of edge of a graph; note that the graphs may also
have parallel edges.

For each input file you should output the size and the number of min cuts. Please upload your code to
Canvas. You should also write the output of your program for each input in the designated ``text box''
of Problem 2 in Canvas. There are four inputs uploaded to the course website. The last one, ``b3.in'' is
very large but it has only 10 percent of the grade of this problem.
\end{problem}

\begin{solution}[Solution]

We construct a weighted adjacency matrix \( G \) and keep track of vertex degrees at each step.

In Karger's algorithm a edge is selected at random uniformly from the edge set.
To do this we first select a vertex \( v_0 \) out of all vertices, with probability proportional to the vertex degree. We then select a vertex \( v_1 \) from the set of vertices attached to \( v_0 \) with probability proportional to the edge weight.

We then merge \( v_1 \) into \( v_0 \) by updating \( G \) and deleting any loops which may have formed.

To keep track of which vertices belong to the same supernode we keep a list where the \( i \)-th entry tells us which supernode vertex \( i \) belongs to. Every time we contract an edge we update this list.

\iffalse    
In pseudo-code, our algorithm is as follows.
\begin{algorithm}[h]
\begin{algorithmic}[1]
\Function{Karger}{G,V,vertex-degrees,vertex-labels}
    \While{count-if-nonzero(vertex-degrees) \( >2 \)}
        \State select random vertex \( v_0 \) from \( V \)  weighted proportional to vertex-degrees 
        \State select random vertex \( v_1 \) from \( V \)  weighted proportional to  \( G_{v_1,:} \) 
        \State vertex-labels[\(v_1\)]\( \: = v_0 \)
        \For{\(v\) in \( V\setminus \{v_0,v_1\} \)}
            \State \( G_{v_0,v} \mathrel{+}= G_{v_1,v} \) 
            ,\hspace{1em} \( G_{v,v_0} \mathrel{+}= G_{v,v_1} \)
            \State vertex-degrees[\(v_0\)]\( \: \mathrel{+}= 1 \)
            \State \( G_{v_1,v} = 0 \) 
            ,\hspace{1em} \( G_{v,v_1} = 0 \)
        \EndFor
        \State \( G_{v_0,v_1} = 0 \)
        ,\hspace{1em} \( G_{v_1,v_0} = 0 \)
    \EndWhile
    \State \Return vertex-labels,vertex-degrees
\EndFunction
\end{algorithmic}
\end{algorithm}
\fi

The size of the max cut will be the maximum value of vertex-degrees, and the cut itself can be obtained from vertex-labels.

We implement this in python with a few modifications. 
    First, the reading and writing to \( G \) are vectorized. As a result numpy will use its compiled functions to compute these changes. We also update any of the entries of vertex-label which were previously \( v_1 \) to \( v_0 \). This way the final output of vertex-label will have just two values. 
    We also note that \( G \) is symmetric so that we could avoid about half the read/writes to \( G \) by using only the upper triangle.


This implementation is fast enough that for the first three data sets we are able to run it \( \cO(n^2) \) in under a minute to find all min cuts. However, to find a single cut on the final data set takes about 2-3 seconds, so to find \( n^2 = 10000^2 \) cuts is not tractable.

We therefore implement the Karger-Stein algorithm. At each stage we take the submatrix generated by the nonzero entries of the vertex degree vector as input to the recursive call. We also keep track of all cuts corresponding to the smallest cut found in a given round

\lstinputlisting{p1_data/KS.py}

We then post process the output, finding the size of the min cut and then creating a set of all of the min cuts.

\lstinputlisting{p1_data/find_cuts.py}


Figure~\ref{results} shows the result of running our algorithm on the data sets provided. 

The total number of min cuts found is provided to give some sense of how likely we are to have found all the cuts.

We note that we did not find a cut of size smaller than 100 on b3 despite running over 100 iterations of Karger-Stein. We also note that we did not save any cuts of size 100 since this required a lot of IO and we did not expect 100 to be the min cut.

\begin{table}\centering
\begin{tabular}{|r||l|l|l|} \hline
    dataset & min cut size & \# of unique min cuts found & total \# of min cuts found \\ \hline
    b0 & 8 & 50 & 27277 \\ \hline
    b1 & 2 & 74 & 89101 \\ \hline
    b2 & 1 & 3 & 68334 \\ \hline
    b3 & 100 & & \\ \hline
\end{tabular}
    \caption{Summary of Results}
    \label{results}
\end{table}



\end{solution}

\begin{problem}[Problem 2]
    Consider the following process for executing \( n \) jobs on \( n \) processors. In each round, every (remaining)
job picks a processor uniformly and independently at random. The jobs that have no contention on the
processors they picked get executed, and all other jobs back off and then try again. Jobs only take one
round of time to execute, so in every round all the processors are available.

For example, suppose we want to run 3 jobs on 3 processors. Suppose in round 1, jobs 1 and 2 choose
the first processor and job 3 chooses the second processor. Then job 3 will be executed and jobs 1 and
2 back off. Suppose in round 2, job 1 chooses the third processor and job 2 chooses the first processor.
Then both of them are executed and the process ends in 2 rounds.

    In this problem we almost show that the number of rounds until all jobs are finished is \( \cO(\log \log n) \) with
high probability.

\begin{enumerate}[nolistsep,label=(\alph*)]
    \item Suppose less than \( \sqrt{n} \) jobs are left at the beginning of some round. Show that for some constant \( c > 0 \), with probability at least \( c  \) no job remains after this round.
    \item In the first round we have \( n \) jobs. Show that the expected number of processors that are picked by no jobs is \( n(1 - 1/n)^n \).
    \item Suppose there are \( r \) jobs left at the beginning of some round. What is the expected number of processors that are matched to exactly one job? What is the expected number of jobs remaining to be completed after that round?
    \item Suppose in each round the number of jobs completed is exactly equal to its expectation. Show that (under this false assumption) the number of rounds until all jobs are finished is \( \cO(\log \log n) \).
    \item In this part we almost justify the false assumption. Suppose there are \( r \) jobs left at the beginning of some round. Let \( E \) be the expected number of processors that are matched to exactly one job. Show that for any \( k > 1 \), the number of processors with exactly one matched job is in the interval \( [E-k\sqrt{r},E+k\sqrt{r}] \) with probability at least \( 1-\exp(-\Omega(k^2)) \). You can use the McDiarmid’s inequality to prove the claim.

\begin{theorem}[McDiarmid’s inequality]
    Let \( X_1,\ldots, X_n\in\cX \) be independent random variables. Let \( f:\cX^n \to \RR \). If for all \( 1\leq i\leq n \) and for all \( x_1,\ldots,x_n \) and \( \tilde{x}_i \),
    \begin{align*}
        |f(x_1,\ldots,x_n) - f(x_1,\ldots,x_{i-1},\tilde{x}_i,x_{i+1},\ldots,x_n)| \leq c_i,
    \end{align*}
    then,
    \begin{align*}
        \PP \left[ |f(X_1,\ldots, X_n) - \EE[f] | \geq \epsilon \right] \leq 2\exp \left( \frac{-2\epsilon^2}{\sum_{i=1}^{n} c_i^2} \right)
    \end{align*}
\end{theorem}
\end{enumerate}
\end{problem}

\begin{solution}[Solution]
\begin{enumerate}[label=(\alph*)]
\item 
    Let \( X_i = \in \{1,\ldots,n\} \) denote the node that processor \( i \) picks. That is, \( X_i \) is uniformly distributed with range \( \{1,\ldots, n\} \).
    
    It is then clear that this is the birthday paradox. It follows immediately from Lemma 3.1 that the probability of no collisions when less than \( \sqrt{n} \) jobs remain is at least one half. If there are no collisions then all jobs terminate in this round. 




    \iffalse

    We compute the probability of no jobs picking the same processor. Denote this probability by \( p(n) \).

    The probability that processor \( i \) picks an empty node given that all previous processes have picked empty nodes is \( (n-(i-1)) / n \).

    Therefore,
    \begin{align*}
        p(n)
        = \prod_{i=1}^{\sqrt{n}} \left( \frac{n-(i-1)}{n} \right)
        = \prod_{i=0}^{\sqrt{n}-1} \left( 1 - \frac{i}{n} \right)
        = 1 - \sum_{i=0}^{\sqrt{n}-1} \frac{i}{n} + \cO\left(\frac{1}{\sqrt{n}}\right)
    \end{align*}

    \textbf{justfy this epxansion?}

    Now note that, 
    \begin{align*}
        \sum_{i=0}^{\sqrt{n}-1} \frac{i}{n} 
        = \frac{1+\sqrt{n}}{2+\sqrt{n}}
    \end{align*}

        It is then clear that \( p(n) \to 1/2 \) as \( n\to\infty \). Since \( n\in\NN \), for any \( \epsilon > 0 \), and in particular for \( \epsilon \in (0,1/2) \), there can only be finitely many \( n \) such that \( |p(n)-1/2| > \epsilon \).
        Moreover, for each of these \( n \), \( p(n) > 0 \). Therefore, \( c := \inf_n p(n) = \min_n p(n) > 0 \) gives a strictly positive lower bound on \( p(n) \) independent of \( n \).
    \fi

\item  
    For a fixed processor \( j \), define,
    \begin{align*}
        \cA_{i\to j} = \text{event that process \( i \) picks processor \( j \)}
    \end{align*}
    
    Then,
        \begin{align*}
            \PP[\cA_{i\to j}] = 1/n
        \end{align*}
        
        Note that \( \cA_{i\to j} \) and \( \cA_{i'\to j} \) are independent for \( i\neq i' \).
    Therefore,
        \begin{align*}
            \PP[\text{processor \(j\) never picked}] = \prod_{i=1}^{n}\PP[\cA_{i\to j}^c] = (1-1/n)^n
        \end{align*}

    
    Let \( X_j = \bOne[\text{processor \( j \) never picked}] \) so that \( X = \sum_{j} X_j \) gives the number of processors never picked. Then,
        \begin{align*}
            \EE[X] = \sum_{i=1}^{n} \EE[X_i] 
            = n \PP[\text{processor \( j \) never picked}]
            = n (1-1/n)^n
        \end{align*}
       

\item


    For a fixed processor \( j \) define,
        \begin{align*}
            \cB_{i\to j} = \text{event that process \( i \) is the only process picking processor \( j \)}
        \end{align*}
        
    Let \( R \) be the set of \( r \) nodes. Then,
        \begin{align*}
            \cB_{i\to j}
            = \cA_{i\to j} \cap \left( \bigcap_{i'\in R\setminus\{i\}} \cA_{i'\to j}^{c} \right)
        \end{align*}

    Again by the independence of \( \cA_{i\to j} \) and \( \cA_{i'\to j} \) for \( i\neq i' \),
        \begin{align*}
            \PP[\cB_{i\to j}]
            %=\PP[\text{process \( i \) is the only job on processor \( j \)}]
            = \PP[\cA_{i\to j}] \left( \prod_{i'\in R\setminus \{i\}} \PP[\cA_{i'\to j}^c] \right)
            = (1/n)(1-1/n)^{r-1} 
        \end{align*}
    
    Therefore, since \( \cB_{i\to j} \) and \( \cB_{i' \to j} \) are independent for \( i\neq i' \),
    \begin{align*}
        \PP[ \text{processor \( j \) has exactly one process} ] 
        = \sum_{i\in R} \PP[\cB_{i\to j}]
        = \frac{r}{n} (1-1/n)^{r-1}
    \end{align*}
    
    Let \( Y_j = \bOne[\text{processor \( j \) has exactly one process}] \) so that \( Y = \sum_{j} Y_j \) gives the number of processors with exactly one process.

    Then,
    \begin{align*}
        \EE[Y] 
        = \sum_{j=1}^{n} \EE[Y_j] 
        = n\PP[\text{processor \( j \) has exactly one process}] 
        = r(1-1/n)^{r-1}
    \end{align*}
    

    Therefore, since a job is run only if it is the only job on a processor,
    \begin{align*}
        \EE[\text{number of jobs remaining}] 
        = r - r(1-1/n)^{r-1} 
    \end{align*}      
    
\item 
    Let \( r_k \) be the number of steps at step \( k \). We seek \( k \) such that \( r_k = 0 \) starting with \( r_0 = n \).
    By the previous result, \( r_k \) is defined recursively as,
    \begin{align*}
        r_{k+1} = r_k - r_k(1-1/n)^{r_k-1}
    \end{align*}

    It is well known that for \( b>1 \) and \( a>0 \) sufficiently small,
    \begin{align*}
        (1-a)^b \geq 1-ab
    \end{align*}

    Therefore, for sufficiently large \( n \) and since \( r_k \leq r_k^2 \),
    \begin{align*}
        r_{k+1} \leq r_k - r_k\left(\frac{1-r_k/n}{1-1/n}\right)
        = \frac{r_k(r_k-1)}{n-1}
        \leq \frac{r_k^2-r_k}{n}
        \leq \frac{2}{n}r_k^2
    \end{align*}
   

    Using Mathematica we can solve recursive relationship \( a_{k+1} =  2a_k^2/n \) with \( a_0 = n/4 \). This has explicit solution,
    \begin{align*}
        a_{k} = n\cdot 2^{-(1+2^k)} 
    \end{align*}
 
    We find \( k \) such that \( a_k = 1 \). We have,
    \begin{align*}
        -(1+2^k) \log(2) = \log \left( 2^{-(1+2^k)} \right) = \log(1/n) = -\log(n)
    \end{align*}

    Therefore,
    \begin{align*}
        2^k  = \log(n) / \log(2) - 1
    \end{align*}
    
    Finally,
    \begin{align*}
        k = \log \left( \frac{\log(n)}{\log(2)} - 1 \right) = \cO(\log \log(n))
    \end{align*}
    
    We now justify that the convergence of \( r_k \) is at least as fast as \( a_k \). 
    
    Obviously once \( r_k = 1 \), the algorithm will terminate in one step (constant time). More importantly, for sufficiently large \( n \), \( r_k \leq n/4 \) in \( \cO(1) \) operations.

    Note that \( \lim_{n\to\infty} (1-1/n)^{n-1} = 1/e \geq 1/4 \). Therefore, 
    \begin{align*}
        \lim_{n\to\infty} \frac{n-n(1-1/n)^{n-1}}{(3/4)n} = \frac{4}{3} \left( 1 - \frac{1}{e} \right) < 1
    \end{align*}

    This shows that for sufficiently large \( n \) less than \( 3/4 \) of the jobs remain after the first step. 
    
    As \( r \) decreases the fraction of the jobs terminating each step increases, since \( r-r(1-1/n)^{r-1} \) is a increasing convex function of \( r \).
    Therefore, in the first 5 iterations, at least \( (3/4)^5 < 1/4 \) of the jobs terminate.


    In summary, \( r_k \) will reach \( n/4 \) is constant time. From this point \( r_k \) converges at least as fast as \( a_k \), which converges like \( \log \log n \). This convergence continues until \( r_k = 1 \), in which case the algorithm terminates in the next step. Therefore, the overall convergence is \( \log\log n \).


\item Define \( f \) by,
    \begin{align*}
        f(x_1,\ldots, x_n) = \sum_{i=1}^{n} \bOne[\forall j \in\{1,\ldots,n\} \setminus\{i\}: x_i \neq x_j ]
    \end{align*}
    That is, \( f \) counts how many of the \( x_i \) are unique.
    
    Then for any \( x_1,\ldots, x_n \) and \( \tilde{x_i} \), \( f \) satisfies,
    \begin{align*}
        |f(x_1,\ldots,x_n) - f(x_1,\ldots,x_{i-1},\tilde{x}_i,x_{i+1},\ldots,x_n)| \leq 2
    \end{align*}

    Let \( X_i \) be the index of the processor process \( i \) picks. Therefore the \( X_i \) are independent and \( X = f(X_1,\ldots, X_N) \) is the total number of processors with exactly one job. Finally,
    \begin{align*}
        E = \EE[f(X_1,\ldots, X_N)]
    \end{align*}
    
    By McDiarmids inequality,
    \begin{align*}
        \PP[|X - \EE[X]| \geq k\sqrt{r}] \leq 
        2 \exp \left( \frac{-2 r k^2}{\sum_{i=1}^{n}2} \right)
        = 2 \exp \left( \frac{-2 r k^2}{2n} \right)
        = \exp \left( \frac{-2 r k^2}{2n} + \ln(2) \right)
    \end{align*}

    Therefore, for fixed \( r \) and \( n \),
    \begin{align*}
        \PP[X \in [E-k\sqrt{r},E+k\sqrt{r}]] \geq 1 - \exp \left( \frac{-2 rk^2}{2n} + \ln(2)\right)
        = 1 - \exp (- \Omega(k^2))
    \end{align*}
    
 %   Now note that for any \( a < 1 \) and \( x \) sufficiently large, \( 2\exp(x) < \exp(ax) \).

 %   This proves that the probability that the number of processors with exactly one job is within \( k\sqrt{r} \) of the expected number of processors with at least one job is \( 1-\exp(-\Omega(k^2)) \) (for fixed \( r \) and \( n \)).

\end{enumerate}
\end{solution}

\begin{problem}[Problem 3]
    Given a graph \( G = (V,E) \) with \( n = |V| \) vertices, let \( k \) be the size of the minimum cut of \( G \). In this problem we show that if \( k \) is large enough we can down sample \( G \) and preserve the size of the min-cut. 

    Let \( H \) be a subgraph of \( G \) defined as follows: For every edge \( e \) of \( G \) include \( e \) in \( H \) with probability \( p = (12\ln n)/ (k\epsilon^2) \). If we include \( e \) in \( H \) we weight it by \( 1/p \). 

    We show that the (weighted) min-cut of \( H \) is at least \( k(1-\epsilon) \) with probability at least \( 1-1/n \). Note that \( H \) has only \( p|E| \) many edges (in expectation). So, \( H \) has significantly less edges than \( G \) if \( k\gg \ln(n)/\epsilon^2 \).

\begin{enumerate}[nolistsep,label=(\alph*)]
    \item For a cut \( (S,\overline{S}) \), let
        \begin{align*}
            w_H(S,\overline{S}) = \frac{|H(S,\overline{S})|}{p}
        \end{align*}
        be the sum of the weights of all edges of \( H \) across the cut. Show that for any set \( S\subset V \), 
        \begin{align*}
            \EE \left[ w_H(S,\overline{S}) \right] = |E(S,\overline{S})|
        \end{align*}
    \item Use Chernoff bound to show that for any set \( S\subset V \),
        \begin{align*}
            \PP \left[ w_H(S,\overline{S}) < (1-\epsilon) | E(S,\overline{S}) | \right] \leq e^{-6\ln n | E(S,\overline{S})|/k} = n^{-6|E(S,\overline{S})|/k}
        \end{align*}
    \item Recall that in class we proved that each graph has at most \( \binom{n}{2} \) many min-cuts. We say a cut of \( G \) is an \( \alpha \)-min-cut if it is of size at most \( \alpha \) times the size of the min cut, i.e. at most \( \alpha k \) many edges. 
        It follows by an extension of the Karger’s algorithm that any graph \( G \) has at most \( n^{2 \alpha} \) \( \alpha \)-min cuts.
        Use union bound (and the latter fact) to show that for any integer \( \alpha \geq 1 \), with probability at least \( 1-1/n^2 \), for all sets \( S \) where \( \alpha k \leq |E(S,\overline{S})|\leq 2 \alpha k \),
        \begin{align*}
            w_H(S,\overline{S}) \geq (1-\epsilon)|E(S,\overline{S})|.
        \end{align*}
    \item Use another application of union bound to show that with probability at least \( 1-1/n \), for all sets \( S\subset V \),
        \begin{align*}
            w_H(S,\overline{S}) \geq (1-\epsilon)|E(S,\overline{S})|.
        \end{align*}
\end{enumerate}
\end{problem}

\begin{solution}[Solution]
    For notational convenience we define \( E_S = E(S,\overline{S}) \). Note also that when we say \( S\subset V \) we generally mean \( S \subset V \) for which \( S\neq \emptyset \) and \( S \neq V \). That is, \( S\in 2^V \setminus \{\emptyset, V\} \).
\begin{enumerate}[label=(\alph*)]
    \item Let \( S \subset V \). 
        Not that \( |H(S,\overline{S})| \) is the number of edges of \( G \) contained in the edge set of \( H \) after (randomly) down sampling.
        We can think of \( |H(S,\overline{S})| \) as a random variable defined by,
        \begin{align*}
            |H(S,\overline{S})| = \sum_{e\in E_S}  X_e
        \end{align*}
        where the \( X_e \) are iid Bernouli random variables with success probability \( p \). 

        Clearly each such variable indicates whether or not edge \( e \) is contained in the edge set of \( H \). 
        
        Then,
        \begin{align*}
            \EE \left[w_H(S,\overline{S}) \right] 
            = \frac{1}{p} \EE \left[ |H(S,\overline{S})| \right] 
            = \frac{1}{p} \left( \sum_{e\in E_S} \PP[e\in H] \right)
            = \frac{1}{p} |E(S,\overline{S})| 
            = |E(S,\overline{S})|
        \end{align*}
    \item 
        Suppose \( Y_i \) are independent and Bernouli distributed (not necessarily identically). %with success probability \( p_i \). 
        Let \(  Y = \sum_{i=1}^{N} Y_i \). Then the Chernoff bound states, 
        \begin{align*}
            \PP(Y \leq (1- \epsilon) \EE[Y]) \leq \exp \left( \frac{- \EE[Y] \epsilon^2}{2} \right)
        \end{align*}
        
       Fix some \( S\subset V \). We can take \( Y_i = X_e \), implicitly using an abritrary bijection between \( i \in\{1,\ldots, N\} \) and \( e\in E(S,\overline{S}) \). Then \( Y = p \: w_H(S,\overline{S}) \) so,
        \begin{align*}
            \EE[Y] = p \: \EE[w_H(S,\overline{S})] 
            =p \: |E(S,\overline{S})| 
        \end{align*}
        

    Applying this bound gives,
    \begin{align*}
        \PP[ w_H(S,\overline{S}) \leq (1-\epsilon) |E(S,\overline{S})|]
        &= \PP[ p \: w_H(S,\overline{S}) \leq (1-\epsilon) p\: |E(S,\overline{S})|]
        \\&\leq \exp \left( \frac{ -p\:|E(S,\overline{S})| \: \epsilon^2}{2}  \right)
        \\&= \exp \left( \frac{-6 \ln n \:|E(S,\overline{S})|}{k} \right)
    \end{align*}


    \item
        Fix an integer \( \alpha \geq 1 \) and for notational convenience define,
        \begin{align*}
            2^V_{\alpha} = \{ S \subset V : |E(S,\overline{S})| \in [\alpha k, 2 \alpha k] \} 
        \end{align*}     
        Note that by the extension of Karger's algorithm, the maximum possible number of \( 2 \alpha \) min cuts is \( n^{4 \alpha} \) so \( |2^V_{\alpha}| \leq n^{4 \alpha} \).
        
        Let \( \cE_S \) be the event that for a fixed \( S \in 2^V \),
        \begin{align*}
            \{ w_H(S,\overline{S}) \leq (1-\epsilon) |E(S,\overline{S})| \}
        \end{align*}
        
        For \( S\in 2^V_{\alpha } \) we have \( |E(S,\overline{S})| \in [\alpha k, 2 \alpha k] \) so,
        \begin{align*}
            \PP[\cE_S] 
            \leq n^{\frac{-6 |E(S,\overline{S})|}{k}}
            \leq n^{\frac{-6 (\alpha k)}{k}}
            = n^{-6 \alpha }
        \end{align*}
       
        Now note that \( \cE^{\alpha} = \cup_{S \in 2^V_{\alpha}} \cE_S \) is the event,
        \begin{align*}
            \left\{ \exists S\in 2^V_{\alpha} : w_H(S,\overline{S}) \leq (1-\epsilon) |E(S,\overline{S})|  \right\}
        \end{align*}
        
        Therefore, using union bound, 
        \begin{align*}
            \PP[\cE^{\alpha}] 
            = \PP \left[\bigcup_{S \in 2^V_{\alpha}} \cE_S \right]
            \leq \sum_{S\in 2^V_{\alpha}} \PP[\cE_S]
            \leq \sum_{S\in 2^V_{\alpha}} n^{-6 \alpha} 
            =  |2^V_{\alpha}| \: n^{-6 \alpha}
        \end{align*}
        
        Using the fact that \( |2^V_{\alpha}| \leq n^{4 \alpha} \),
        \begin{align*}
            \PP \left[ \cE^{\alpha} \right]
            \leq|2^V_{\alpha}| \: n^{-6 \alpha}
            \leq n^{4 \alpha} \: n^{-6 \alpha}
            = n^{-2 \alpha}
        \end{align*}
        
        Therefore, since \( \alpha \geq 1 \),
        \begin{align*}
            \PP \left[ \forall S\subset 2^V_{\alpha} : w_H(S,\overline{S}) \geq (1-\epsilon) |E(S,\overline{S})| \right] 
            = \PP\left[ \: \overline{\cE^{\alpha}} \: \right]
            \geq 1 - \frac{1}{n^{2 \alpha}}
            \geq 1 - \frac{1}{n^2}
            %~\Big|~ |E(S,\overline{S})| \in [ \alpha k, 2 \alpha k] 
        \end{align*}

       

    \item

        \textit{Intuition}: We will partition \( S\subset V \) into sets \( [\alpha k,2 \alpha k] \) for varying \( \alpha \). Applying union bound to the union of the events that \( S \) is in a given set and \( w_H(S,\overline{S}) \) is the right size will give the result.

        Note that we must bound the size of this union, and therefore the number of intervals needed. In particular, we must bound \( |E(S,\overline{S})| \). If \( G \) has parallel edges, there is no bound based on just \( n \) and \( k \) for \( |E(S,\overline{S})| \). To see this consider an example graph where there are three vertices \( 1,2,3 \) and one edge \( \{1,2\} \) and \( m \) parallel edges \( \{2,3\} \). Then \( n=3 \), \( k=1 \), but the max cut is \( m \) which is arbitrary.
We need that the number of intervals required is \( \cO(n) \). 

        
        We therefore make the assumption that the max cut is \(  2^{(n+1)k} \) so that,
        \begin{align*}
            k \leq |E(S,\overline{S})| \leq 2^{(n+1)k}
        \end{align*}
 
        Now observe,
        \begin{align*}
            \bigcup_{i=0}^{n} \left[ 2^{ik},2^{(i+1)k} \right]
            = [2^0, 2^{(n+1) k}] 
        \end{align*}
        so that for any \( S\subset V \),
        \begin{align*}
            \exists \: i \in \{1,\ldots, n \} \text{ such that } |E(S,\overline{S})| \in[ 2^{i k}, 2^{(i+1) k}]
        \end{align*}

        Again let,
        \begin{align*}
            \cE^{\alpha} = \left\{ \exists S\in 2^V_{\alpha} : w_H(S,\overline{S}) \leq (1-\epsilon) |E(S,\overline{S})|  \right\}
        \end{align*}
        
        Now note that, 
        \begin{align*}
            \cE = \bigcup_{i = 0}^{n} \cE^{2^i} 
            =\left\{ \exists S\subset V : w_H(S,\overline{S}) \leq (1-\epsilon) |E(S,\overline{S})|  \right\}
        \end{align*}
 
        Therefore, using union bound and the result from (c), 
        \begin{align*}
            \PP[\cE] 
            = \PP \left[\bigcup_{i = 0}^{n} \cE^{2^i} \right]
            \leq \sum_{\alpha=1}^{n} \PP[\cE^{2^i}]
            \leq \sum_{\alpha=1}^{n} \frac{1}{n^2} 
            =  \frac{n}{n^2}
            \leq \frac{1}{n}
        \end{align*}
        
        Therefore,
        \begin{align*}
            \PP \left[ \forall S\subset V: w_H(S,\overline{S}) \geq (1-\epsilon) |E(S,\overline{S})| \right] 
            = \PP[\overline{\cE}]
            \geq 1 - \frac{1}{n}
        \end{align*}

        \textit{Remark}: If we have a better bound for the max cut we can improve the bound. For instance, if there are no parallel edges we can lazily bound the max cut by \( n^2 \) and find a bound of nearly \( 1-1/n^2 \). Moreover, if use the bound of \( 1/n^{2 \alpha} \) for \( \PP[\cE^{\alpha}] \) then a better bound can be obtained. 


\end{enumerate}

\end{solution}


\begin{problem}[Problem 4]
    In lecture 4 we discussed the pairwise independent hash functions. We say that for a prime \( p \) we can 
    generate a pairwise independent hash functions by choosing \(a\), \(b\) independently from the interval \( \{1, \ldots, p-1 \} \) 
    and using \( ax + b \) as a random number. Suppose we generate \( t \) pseudo random numbers this way,
    \( r_1, \ldots , r_t \) where \( r_i = a i + b \). We want to say this set is far from being mutually independent. Consider
    the set \( S = \{p/2, \ldots , p - 1\} \) which has half of all elements. Prove that with probability at least \( \Omega(1/t) \)
    none of the pseudo-random-numbers are in \( S \). Note that if we had mutual independence this would have been \( 1/2^t \).
\end{problem}

\begin{solution}[Solution]
First note that \( r_i \) are elements of the finite field \( \ZZ_p \) and \( a\neq 0 \) so that \( a \) is (uniquely) invertible. Then,
\begin{align*}
    r_i - r_j = a(i-j) = 0
    && \Longleftrightarrow &&
    i = j
\end{align*}

Therefore, by the pigeon hole principle, if \( t > p/2 \), then there is some \( r_i \) contained in \( S \). That is to say, the problem only makes sense when \( t  \) is much smaller than \( p \). We therefore consider the case where \( t \to T \) and \( T\ll p \).

    \textit{Intuition}: If \( a \) is near 0 (where by near we mean that \( p-1 \) and \( 1 \) are equidistant to zero) then the \( r_i \) will be closely clustered. Therefore, if \( b \) is far from \( S \) (i.e. near \( p /4 \)) we do not expect any of the \( r_i \) to be in \( S \).

Figure~\ref{ok} shows an example of a choice of \( a \) and \( b \) for which \( t=4 \) and in fact \( t=5 \) produce ``random numbers'' not in \( S \) (highlighted in red). Similarly, Figure~\ref{notok} shows a choice of \( a \) and \( b \) for which even \( t=2 \) produces some ``random numbers'' in \( s \).

\begin{figure}[h]\centering
\begin{subfigure}{.48\textwidth}\centering
    \begin{tikzpicture}[scale=.8]
    \def\p{37};
    \fill[fill=red,opacity=.2] (3.088,-.262) arc (-4.86:-180:3.1) -- (-2.7,0) arc (-180:-4.86:2.7) -- (3.088,-.262); 
    \foreach \i in {0,...,36}{
        \draw[line width=1pt] (360*\i/\p:2.8cm) -- (360*\i/\p:3cm);
    }
    \node[] at (0:3.4cm) {\(0\)}; 
    \node[] at (180:3.6cm) {\(p/2\)}; 
    \def\a{2};
    \def\b{8};
    \node[] at (360*8/\p:3.5cm) {\(b\)}; 
    \foreach \i in {8,10,12,14}{%{\b+\a,\b+2*\a,\b+3*\a,\b+4*\a}{
        \draw[fill=blue] (360*\i/\p+360*2/\p:2.9cm) circle (.1cm);
        \draw[->] (360*\i/\p:3.2cm) arc (360*(\i+.05)/\p:360*(\i+2-.05)/\p:3.2cm);
        \node[] at (360*\i/\p + 360/\p:3.45cm) {\(+a\)};
    }
\end{tikzpicture}
\caption{\(p=37,a=2,b=8,t=4\)}
\label{ok}
\end{subfigure}\hfill
\begin{subfigure}{.48\textwidth}\centering
    \begin{tikzpicture}[scale=.8]
    \def\p{37};
    \fill[fill=red,opacity=.2] (3.088,-.262) arc (-4.86:-180:3.1) -- (-2.7,0) arc (-180:-4.86:2.7) -- (3.088,-.262); 
    \foreach \i in {0,...,36}{
        \draw[line width=1pt] (360*\i/\p:2.8cm) -- (360*\i/\p:3cm);
    }
    \node[] at (0:3.4cm) {\(0\)}; 
    \node[] at (180:3.6cm) {\(p/2\)}; 
    \def\a{2};
    \def\b{6};
    \node[] at (360*6/\p:3.5cm) {\(b\)}; 
    \foreach \i in {6,16,26}{%{\b+\a,\b+2*\a,\b+3*\a,\b+4*\a}{
        \draw[fill=blue] (360*\i/\p+10*360/\p:2.9cm) circle (.1cm);
        \draw[->] (360*\i/\p:3.2cm) arc (360*(\i)/\p:360*(\i+10)/\p:3.2cm);
        \node[] at (360*\i/\p + 5*360/\p:3.45cm) {\(+a\)};
    }
\end{tikzpicture}
\caption{\(p=37,a=10,b=6,t=3\)}
\label{notok}
\end{subfigure}
\end{figure}

We now prove this formally. Given that \( a,b\in \{1,\ldots,p-1\} \) there are \( (p-1)^2 \) values of \( a \) and \( b \). For a fixed \( t < T \ll p \) we give a bound on how many of these pairs of \( a \) and \( b \) will produce numbers \( r_1,\ldots, r_t \) not in \( S \).

Define \( \norm{\cdot}: \ZZ_p \to \NN_{\geq 0} \) to be the absolute distance to \( 0 \) in \( \ZZ_p \). That is,
\begin{align*}
    \norm{x} = \min_{k\in\ZZ} | i(x) - p k|
\end{align*}
where \( i:\ZZ_p\to\RR \) is the natural inclusion map and \( |\cdot| \) is taken over \( \RR \).

With this notation, \( \norm{1} = \norm{p-1} = 1 \), \( \norm{2} - \norm{p-2} = 2 \) etc.


Fix \( t \ll p \) and define \( A = \{a : \norm{a} <  p/(2(t-1)) \} \). Then all of the \( a \) which do not produce a collision with \( S \) are contained in \( A \).


To see this observe that if \( \norm{a} > p/(2(t-1)) \) that one of the \( r_i \) will be in \( S \) regardless of \( b \) since to get from \( r_1 \) to \( r_t \), passing in order through each of the other points \( r_2, \ldots, r{t-1} \) we must pass over about \( \norm{a}(t-1) > p/2 \) points. This means that at least some of them are in \( S \).


%\( r_1 = a+b \) and \( r_t = a t +b = p/2 + b \) (note that we have used the assumption that \( t \ll p \) implicitly by assuming . 

Suppose that \( a\in A \). The range spanned by the \( r_i \) is of size \( \norm{a}(t-1) \). There are about \( (p+1)/2 \) elements not in \( S \), so there are \( (p+1)/2 - \norm{a}(t-1) \) possible values of \( b \) for which none of the \( r_i \) are in \( S \).

Therefore, the total number of pairs \( a,b \) for which none of the \( r_i \) are in \( S \), denoted \( N \) satisfies,
\begin{align*}
    N &\geq \sum_{a\in A } \left( \frac{p}{2} - \norm{a}(t-1) \right)
    = 2\sum_{a=1}^{\lceil p/(2(t-1)) \rceil} \left( \frac{p}{2} - a(t-1) \right)
    \geq 2 \left\lceil \frac{p}{2(t-1)} \right\rceil \frac{p}{2}
    %= \frac{p^2 t+p^2-2 p t^2+2 p t}{4 t^2}
\end{align*}

\iffalse
Using the identity \( 1+2+\cdots+k = k(k+1)/2 \), we have,
\begin{align*}
    N
    \geq 2 \left\lceil \frac{p}{2t} \right\rceil \frac{p}{2} - 2 (t-1) \frac{1}{2} \left\lceil \frac{p}{2 t} \right\rceil \left( 1+ \left\lceil \frac{p}{2t} \right\rceil \right) 
    \geq \frac{p^2}{2 t}
\end{align*}
\fi

The probability of picking such \( a \) and \( b \) is,
\begin{align*}
    \PP \left[ \frac{\text{number of }a,b \text{ that do not produce collision with }S}{\text{number of possible choices of }a,b} \right]
    = \frac{N}{(p-1)^2} = \Omega \left( \frac{1}{t} \right)
\end{align*}







\end{solution}



\begin{problem}[Extra Credit]
    Say we have a plane with \( n  \) seats and we have a sequence of \( n \) passengers \( 1, 2, \ldots , n \) who
    are going to board the plane in this order and suppose passenger \( i \) is supposed to sit at seat \( i \). Say when
    1 comes they chooses to sit at some arbitrary other seat different from 1. From now on, when passenger \( i \)
    boards, if their seat \( i  \) is available they sit at \( i \), otherwise they choose to sit at a uniformly random seat that is
    still available. What is the probability that passenger \( n \) sits at their seat \( n \)?
\end{problem}

\begin{solution}[Solution]

    When passenger \( i \) boards, all the seats of index \( j \), \( 1<j<i \) must be filled by the boarding rules (if seat \( j \) were empty, then person \( j \) did not take their seat when they could have). There are \( i-2 \) such seats, and \( i-1 \) passengers already seated. Therefore, if someone is in seat one, seat \( i \) must be empty and person \( i \) (and, by induction, everyone behind them) can sit in their own seat.

    We therefore make the key observation that the success or failure of the process is fixed once a person sits in seat 1 or \( n \) (by success we mean person \( n \) sits in seat \( n \), and by failure we mean they do not). Moreover, before the \( n \)-th passenger boards, someone must have sat in at least of of seat 1 or seat \( n \). Therefore we need only consider the boarding process until seat 1 or seat \( n \) is taken.
    
    If person 1 sits in seat \( n \), then person \( n \) will sit in seat \( n \) with probability zero.

    Suppose person 1 does not sit in seat \( n \). By construction, if seats 1 and \( n \) are available (i.e. the success or failure of the process has not been determined), the probability that any person sits in seat 1 is the same as the probability that they sit in seat \( n \). 
    
    Therefore, given that person one does not sit in seat \( n \), the probability that seat 1 is filled before seat \( n \) is the same as the probability that seat \( n \) is filled before seat 1. As mentioned above, these events correspond exactly to person \( n \) sitting or not sitting in seat \( n \), and the union of the events is the entire probability space. 

    If person 1 does not sit in seat \( n \), then person \( n \) will sit in seat \( n \) with probability \( 1/2 \).


    \iffalse
Let \( p\) be the probability that person \( n \) sits in their own seat. 

Note that the success or failure of the process is fixed once a person sits in seat 1 or \( n \). Moreover, before the \( n \)-th passenger boards, someone must have sat in seat 1 or seat \( n \).

Note also that given person 1 does not sit in seat \( n \), the probability that any person sits in seat 1 is the same as the probability that they sit in seat \( n \). Therefore, given that person one does not sit in seat \( n \), the probability that seat 1 is filled before seat \( n \) is the same as the probability that seat \( n \) is filled before seat 1.

Using \( 1\to n \) to denote the event ``passenger one sits in seat \( n \)'',
\begin{align*}
    1 = \PP(1\to n) + \PP(1\not\to n) \left( \PP(1\text{ filled before }n | 1\not\to n) + \PP(n\text{ filled before 1} | 1\not\to n) \right)
\end{align*}

Note that by the definition of conditional probability, and since passenger 1 cannot sit in seat \( n \) if seat 1 is filled before seat \( n \),
\begin{align*}
    \PP(1\not\to n) \PP(1\text{ filled before }n | 1\not\to n)
    = \PP(1\text{ filled before }n)
    = p
\end{align*}

    By the symmetry argument above,
\begin{align*}
    \PP(1\text{ filled before }n | 1\not\to n)
    =
    \PP(n\text{ filled before }1 | 1\not\to n)
\end{align*}

Therefore,
\begin{align*}
    \frac{1}{n-1} + p + p= 1 
    && \Rightarrow &&
    p = \frac{n-2}{2(n-1)}
\end{align*}
\fi

\end{solution}


\end{document}
